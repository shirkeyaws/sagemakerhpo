{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Licensed under the Amazon Software License (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at: http://aws.amazon.com/asl/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning on SageMaker XGBoost algorithm\n",
    "\n",
    "This sample notebook shows how to use [Amazon SageMaker's built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to predict whether a driver will file an auto insurance claim next year, based on a public dataset provided by an insurance company on Kaggle (To get more information about the dataset, please visit https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data). It leverages hyperparameter tuning to automatically kick off traning jobs with different hyperparameter combinations, to find the one with best model training result.\n",
    "\n",
    "You can certainly use your own dataset, in which case you simply change the training data location to your own s3 bucket, as you will see later in the notebook. \n",
    "\n",
    "After the tuning job is completed, we will also show you how to deploy the best model and make predictions against the endpoint, which you can find in other SageMaker sample notebooks as well.\n",
    "\n",
    "---\n",
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services.\n",
    "\n",
    "#### Get the HPO client, which is region specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import smhpolib\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "account = boto3.Session().client('sts').get_caller_identity()['Account']\n",
    "sagemaker = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the execution role that is to be passed to training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::306280812807:role/service-role/AmazonSageMaker-ExecutionRole-20180117T091311\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify s3 bucket and prefix\n",
    "Set up the S3 bucket that you want to use for putting training data and model data. In this example, you use a public dataset that is stored in a public S3 bucket that we prepared. If you want to use your own datasets, you can put your datasets in teh bucket you specify here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-306280812807'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'sagemaker-{}-{}'.format(region, account)    # put your s3 bucket here\n",
    "prefix = 'hpo/xgboost'       # specify the s3 prefix (i.e., subfolder) for this exercise\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify hyperparameter tuning job configuration\n",
    "Now you configure the tuning job by defining a JSON object that you pass as the value of the TuningJobConfig parameter to the create_tuning_job call. In this JSON object, you specify:\n",
    "* The ranges of hyperparameters you want to tune\n",
    "* The limits of the resource the tuning job can consume \n",
    "* The objective metric for the tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-tuningjob-22-05-31-55\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "tuning_job_name = 'xgboost-tuningjob-' + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "print (tuning_job_name)\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": { \n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"eta\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"1\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"rate_drop\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"0\",\n",
    "          \"Name\": \"gamma\",\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"min_child_weight\",\n",
    "        }\n",
    "          ,\n",
    "           {\n",
    "          \"MaxValue\": \"2\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"tweedie_variance_power\",\n",
    "        }\n",
    "      ],\n",
    "        \n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"20\",\n",
    "          \"MinValue\": \"5\",\n",
    "          \"Name\": \"max_depth\",\n",
    "        },\n",
    "       {\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\",\n",
    "          \"Name\": \"max_delta_step\",\n",
    "        },\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 20,\n",
    "      \"MaxParallelTrainingJobs\": 3\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"validation:auc\",\n",
    "      \"Type\": \"Maximize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify training job configuration\n",
    "Now you configure the training jobs the tuning job launches by defining a JSON object that you pass as the value of the TrainingJobDefinition parameter to the create_tuning_job call.\n",
    "In this JSON object, you specify:\n",
    "* Metrics that the training jobs emit\n",
    "* The container image for the algorithm to train\n",
    "* The input configuration for your training and test data\n",
    "* Configuration for the output of the algorithm\n",
    "* The values of any algorithm hyperparameters that are not tuned in the tuning job\n",
    "* The type of instance to use for the training jobs\n",
    "* The stopping condition for the training jobs\n",
    "\n",
    "This example defines two metrics the built-in XGBoost Algorithm emits: valid_auc and train_auc. In this example, we set static values for the eval_metric, auc, num_round, objective, rate_drop, and tweedie_variance_power parameters of the built-in XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "           'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "           'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "           'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\n",
    "           \n",
    "training_image = containers[region]\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "      {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": \"s3://public-test-hpo-datasets-{}/kaggle/porto-seguro/xgb/train/\".format(region)\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"CompressionType\": \"None\",\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"DataSource\": {\n",
    "          \"S3DataSource\": {\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": \"s3://public-test-hpo-datasets-{}/kaggle/porto-seguro/xgb/val/\".format(region)\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": \"s3://{}\".format(bucket)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 1,\n",
    "      \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "      \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "      \"eval_metric\": \"auc\",\n",
    "      \"num_round\": \"160\",\n",
    "      \"objective\": \"binary:logistic\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 43200\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and launch a hyperparameter tuning job\n",
    "Now you can launch a hyperparameter tuning job by calling create_tuning_job API. Pass the name and JSON objects you created in previous steps as the values of the parameters. After the tuning job is created, you should be able to describe the tuning job to see its progress in the next step, and you can go to SageMaker console->Jobs to check out the progress of each training job that has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:306280812807:hyper-parameter-tuning-job/xgboost-tuningjob-22-05-31-55',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
       "   'content-length': '130',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 22 May 2018 05:32:14 GMT',\n",
       "   'x-amzn-requestid': '79b3063f-b323-4e60-9ce7-53846d06fd4b'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': '79b3063f-b323-4e60-9ce7-53846d06fd4b',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                            HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                            TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track hyperparameter tuning job progress\n",
    "After you launch a tuning job, you can see its progress by calling describe_tuning_job API. The output from describe-tuning-job is a JSON object that contains information about the current state of the tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CreationTime': datetime.datetime(2018, 5, 21, 14, 40, 43, tzinfo=tzlocal()),\n",
       " 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:306280812807:hyper-parameter-tuning-job/xgboost-tuningjob-21-14-31-09',\n",
       " 'HyperParameterTuningJobConfig': {'HyperParameterTuningJobObjective': {'MetricName': 'validation:auc',\n",
       "   'Type': 'Maximize'},\n",
       "  'ParameterRanges': {'CategoricalParameterRanges': [],\n",
       "   'ContinuousParameterRanges': [{'MaxValue': '1',\n",
       "     'MinValue': '0',\n",
       "     'Name': 'eta'},\n",
       "    {'MaxValue': '10', 'MinValue': '0', 'Name': 'gamma'},\n",
       "    {'MaxValue': '10', 'MinValue': '1', 'Name': 'min_child_weight'}],\n",
       "   'IntegerParameterRanges': [{'MaxValue': '10',\n",
       "     'MinValue': '1',\n",
       "     'Name': 'max_depth'}]},\n",
       "  'ResourceLimits': {'MaxNumberOfTrainingJobs': 20,\n",
       "   'MaxParallelTrainingJobs': 3},\n",
       "  'Strategy': 'Bayesian'},\n",
       " 'HyperParameterTuningJobName': 'xgboost-tuningjob-21-14-31-09',\n",
       " 'HyperParameterTuningJobStatus': 'InProgress',\n",
       " 'LastModifiedTime': datetime.datetime(2018, 5, 21, 14, 40, 47, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
       "   'content-length': '3503',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 21 May 2018 14:40:47 GMT',\n",
       "   'x-amzn-requestid': 'a4155287-1c68-496c-8018-472536f2d6da'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'a4155287-1c68-496c-8018-472536f2d6da',\n",
       "  'RetryAttempts': 0},\n",
       " 'TrainingJobCounters': {'ClientError': 0,\n",
       "  'Completed': 0,\n",
       "  'Fault': 0,\n",
       "  'InProgress': 0,\n",
       "  'Stopped': 0},\n",
       " 'TrainingJobDefinition': {'AlgorithmSpecification': {'MetricDefinitions': [{'Name': 'train:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\]#011train-mae:(\\\\S+).*'},\n",
       "    {'Name': 'validation:auc',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-auc:(\\\\S+)'},\n",
       "    {'Name': 'train:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\]#011train-merror:(\\\\S+).*'},\n",
       "    {'Name': 'train:auc', 'Regex': '.*\\\\[[0-9]+\\\\]#011train-auc:(\\\\S+).*'},\n",
       "    {'Name': 'validation:mae',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mae:(\\\\S+)'},\n",
       "    {'Name': 'validation:error',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-error:(\\\\S+)'},\n",
       "    {'Name': 'validation:merror',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-merror:(\\\\S+)'},\n",
       "    {'Name': 'validation:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-logloss:(\\\\S+)'},\n",
       "    {'Name': 'train:rmse', 'Regex': '.*\\\\[[0-9]+\\\\]#011train-merror:(\\\\S+).*'},\n",
       "    {'Name': 'train:logloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\]#011train-logloss:(\\\\S+).*'},\n",
       "    {'Name': 'train:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\]#011train-mlogloss:(\\\\S+).*'},\n",
       "    {'Name': 'validation:rmse',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-rmse:(\\\\S+)'},\n",
       "    {'Name': 'validation:ndcg',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-ndcg:(\\\\S+)'},\n",
       "    {'Name': 'train:error', 'Regex': '.*\\\\[[0-9]+\\\\]#011train-error:(\\\\S+).*'},\n",
       "    {'Name': 'validation:mlogloss',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-logloss:(\\\\S+)'},\n",
       "    {'Name': 'train:ndcg', 'Regex': '.*\\\\[[0-9]+\\\\]#011train-ndcg:(\\\\S+).*'},\n",
       "    {'Name': 'train:map', 'Regex': '.*\\\\[[0-9]+\\\\]#011train-map:(\\\\S+).*'},\n",
       "    {'Name': 'validation:map',\n",
       "     'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-map:(\\\\S+)'}],\n",
       "   'TrainingImage': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
       "   'TrainingInputMode': 'File'},\n",
       "  'InputDataConfig': [{'ChannelName': 'train',\n",
       "    'CompressionType': 'None',\n",
       "    'ContentType': 'csv',\n",
       "    'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://public-test-hpo-datasets-us-east-1/kaggle/porto-seguro/xgb/train/'}}},\n",
       "   {'ChannelName': 'validation',\n",
       "    'CompressionType': 'None',\n",
       "    'ContentType': 'csv',\n",
       "    'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
       "      'S3DataType': 'S3Prefix',\n",
       "      'S3Uri': 's3://public-test-hpo-datasets-us-east-1/kaggle/porto-seguro/xgb/val/'}}}],\n",
       "  'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-306280812807'},\n",
       "  'ResourceConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.c4.2xlarge',\n",
       "   'VolumeSizeInGB': 10},\n",
       "  'RoleArn': 'arn:aws:iam::306280812807:role/service-role/AmazonSageMaker-ExecutionRole-20180117T091311',\n",
       "  'StaticHyperParameters': {'_tuning_objective_metric': 'validation:auc',\n",
       "   'eval_metric': 'auc',\n",
       "   'num_round': '100',\n",
       "   'objective': 'binary:logistic',\n",
       "   'rate_drop': '0.3',\n",
       "   'tweedie_variance_power': '1.4'},\n",
       "  'StoppingCondition': {'MaxRuntimeInSeconds': 43200}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to check current status of hyperparameter tuning job\n",
    "sagemaker.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call list_training_jobs_for_tuning_job to see a detailed list of the training jobs that the tuning job launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
       "   'content-length': '1080',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 21 May 2018 14:40:58 GMT',\n",
       "   'x-amzn-requestid': 'c7db69e1-25d5-4441-8b4e-5e3c5edb3a9e'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'c7db69e1-25d5-4441-8b4e-5e3c5edb3a9e',\n",
       "  'RetryAttempts': 0},\n",
       " 'TrainingJobSummaries': [{'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:306280812807:training-job/xgboost-tuningjob-21-14-31-09-003-af0c2c92',\n",
       "   'TrainingJobName': 'xgboost-tuningjob-21-14-31-09-003-af0c2c92',\n",
       "   'TrainingJobStatus': 'InProgress',\n",
       "   'TunedHyperParameters': {'eta': '0.18771938145441602',\n",
       "    'gamma': '0.23709200061741154',\n",
       "    'max_depth': '6',\n",
       "    'min_child_weight': '5.648140042272571'}},\n",
       "  {'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:306280812807:training-job/xgboost-tuningjob-21-14-31-09-002-428b2ff3',\n",
       "   'TrainingJobName': 'xgboost-tuningjob-21-14-31-09-002-428b2ff3',\n",
       "   'TrainingJobStatus': 'InProgress',\n",
       "   'TunedHyperParameters': {'eta': '0.025104172593211738',\n",
       "    'gamma': '0.1822902819630401',\n",
       "    'max_depth': '7',\n",
       "    'min_child_weight': '8.39669572093343'}},\n",
       "  {'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:306280812807:training-job/xgboost-tuningjob-21-14-31-09-001-af841436',\n",
       "   'TrainingJobName': 'xgboost-tuningjob-21-14-31-09-001-af841436',\n",
       "   'TrainingJobStatus': 'InProgress',\n",
       "   'TunedHyperParameters': {'eta': '0.5223045334735177',\n",
       "    'gamma': '2.7218249260421445',\n",
       "    'max_depth': '4',\n",
       "    'min_child_weight': '2.4948833420195085'}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# list all training jobs that have been created by the tuning job\n",
    "list_training_result = sagemaker.list_training_jobs_for_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name, MaxResults=20)\n",
    "list_training_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't go beyond here with Run All, when the tuning job is completed, skip this cell and move on\n",
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze tuning job results - after tuning job is completed\n",
    "Once the tuning job is completed (i.e., all training jobs have been finished), we can list hyperparameters and objective metrics of all training jobs and pick up the training job with the best objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from smhpolib import analysis    # analytical library provided through smhpolib, you can find the source code under /smhpolib folder\n",
    "\n",
    "tuning = analysis.TuningJob(tuning_job_name = tuning_job_name)\n",
    "\n",
    "HPO_params = tuning.hyperparam_dataframe()\n",
    "\n",
    "if len(HPO_params) > 0:\n",
    "    df = HPO_params[HPO_params['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=False)\n",
    "        print(\"Valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        best_model = df.iloc[0]\n",
    "        print(\"best model information: \\n%s\" %best_model)\n",
    "        best_training_job_name = best_model['TrainingJobName']\n",
    "        pd.set_option('display.max_colwidth', -1)  # Don't truncate TrainingJobName        \n",
    "    else:\n",
    "        print(\"Training jobs launched are not completed yet. Try again in a few minutes.\")\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See TuningJob results vs time\n",
    "Next we will show how the objective metric changes over time, as the tuning job progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show\n",
    "import bokeh.palettes\n",
    "\n",
    "def big_warp_palette(size, palette_func, warp=1):\n",
    "    \"\"\"setting warp < 1 exagerates the high end.\n",
    "    setting warp > 1 exagerates the low end\"\"\"\n",
    "    p = palette_func(256)\n",
    "    out = []\n",
    "    for i in range(size):\n",
    "        f = i / size # from 0-1 inclusive\n",
    "        f **= warp\n",
    "        idx = int(f * 255)\n",
    "        out.append(p[idx])\n",
    "    return out\n",
    "\n",
    "if len(df) > 0:\n",
    "    palette = big_warp_palette(len(df),bokeh.palettes.plasma, 0.4)\n",
    "    df['color'] = palette\n",
    "    hover = smhpolib.viz.SmhpoHover(tuning)\n",
    "\n",
    "    p = figure(plot_width=900, plot_height=400, tools=hover.tools(), x_axis_type='datetime')\n",
    "    p.circle(source=df, x='TrainingCreationTime', y='FinalObjectiveValue', color='color')\n",
    "    show(p)\n",
    "else:\n",
    "    print(\"Training jobs launched are not completed yet. Try again in a few minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the correlation between objective metric and individual hyperparameters \n",
    "Now you have finished a tuning job, you may want to know the correlation between your objective metric and individual hyperparameters you've selected to tune. Having that insight will help you decide whether it makes sense to adjust search ranges for certain hyperparameters and start another tuning job. For exmaple, if you see a positive trend between objective metric and a numerical hyperparameter, you probably want to set a higher tuning range for that hyperparameter in your next tuning job.\n",
    "\n",
    "The following cell draws a graph for each hyperparameter to show its correlation with your objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which hyperparameters to look for correlations for\n",
    "all_hyperparameters = tuning.hyperparam_ranges().keys()\n",
    "all_hyperparameters\n",
    "\n",
    "figures = []\n",
    "for hp in all_hyperparameters:\n",
    "    p = figure(plot_width=500, plot_height=500, \n",
    "                title=\"Final objective vs %s\" % hp,\n",
    "                tools=hover.tools(),\n",
    "                x_axis_label=hp, y_axis_label=\"objective\")\n",
    "    p.circle(source=df,x=hp,y='FinalObjectiveValue',color='color')\n",
    "    figures.append(p)\n",
    "show(bokeh.layouts.Column(*figures))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the best model\n",
    "Now we are ready to deploy the best model so we can make inferences against it. In order to deploy a model, we have to import the model from training to hosting, then create an endpoint configuration, after that, we create an endpoint using the model and the endpoint configuration we just created.\n",
    "\n",
    "### Import model into hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=best_training_job_name\n",
    "print(model_name)\n",
    "\n",
    "info = sagemaker.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "hosting_image = training_image  # For XGBoost algorithm, training and hosting share the same image\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Now, we'll create an endpoint configuration which provides the instance type and count for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.c5.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sagemaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, you can now validate the model for use by invoking the endpoint you just created and passing in a sample data for prediction\n",
    "\n",
    "### Get some sample data\n",
    "You can simiply use the first row in the validation data, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First row from validation dataset\n",
    "sample_record=\"0,5,1,4,0,0,0,0,0,1,0,0,0,0,0,6,1,0,0,0.9,1.8,2.332648709,10,0,-1,0,0,14,1,1,0,1,104,2,0.445982062,0.879049073,0.40620192,3,0.7,0.8,0.4,3,1,8,2,11,3,8,4,2,0,9,0,1,0,1,1,1\"\n",
    "label,payload = sample_record.split(',',maxsplit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "result = float(result.decode(\"utf-8\"))\n",
    "print ('Label: ',label,'\\nPrediction: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
